{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2929 images belonging to 5 classes.\n",
      "Found 368 images belonging to 5 classes.\n",
      "Found 365 images belonging to 5 classes.\n",
      "Epoch 1/14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3873s\u001b[0m 42s/step - accuracy: 0.4011 - loss: 1.9002 - val_accuracy: 0.5462 - val_loss: 1.8366\n",
      "Epoch 2/14\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3783s\u001b[0m 41s/step - accuracy: 0.6195 - loss: 1.2267 - val_accuracy: 0.5245 - val_loss: 1.6656\n",
      "Epoch 3/14\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5098s\u001b[0m 56s/step - accuracy: 0.6852 - loss: 0.9159 - val_accuracy: 0.7092 - val_loss: 0.7791\n",
      "Epoch 4/14\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3612s\u001b[0m 39s/step - accuracy: 0.6875 - loss: 0.8589 - val_accuracy: 0.4239 - val_loss: 1.3799\n",
      "Epoch 5/14\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9842s\u001b[0m 108s/step - accuracy: 0.7054 - loss: 0.8398 - val_accuracy: 0.6386 - val_loss: 1.7923\n",
      "Epoch 6/14\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11613s\u001b[0m 127s/step - accuracy: 0.7211 - loss: 0.7634 - val_accuracy: 0.7038 - val_loss: 0.9061\n",
      "Epoch 7/14\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9495s\u001b[0m 103s/step - accuracy: 0.7258 - loss: 0.7317 - val_accuracy: 0.7310 - val_loss: 0.9375\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - accuracy: 0.6501 - loss: 1.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8359920978546143\n",
      "Test Accuracy: 0.7205479741096497\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Multiply, Dropout, BatchNormalization, Reshape, Conv2D, MaxPooling2D, Flatten, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Define the squeeze-and-excite block\n",
    "def squeeze_excite_block(input_tensor, ratio=16):\n",
    "    channels = int(input_tensor.shape[-1])\n",
    "    x = GlobalAveragePooling2D()(input_tensor)\n",
    "    x = Dense(channels // ratio, activation='relu')(x)\n",
    "    x = Dense(channels, activation='sigmoid')(x)\n",
    "    x = Reshape((1, 1, channels))(x)  # Reshape to match input tensor shape\n",
    "    x = Multiply()([input_tensor, x])\n",
    "    return x\n",
    "\n",
    "# Define custom preprocessing function for Gaussian noise\n",
    "def apply_gaussian_noise(image):\n",
    "    noise = np.random.normal(loc=0, scale=0.1, size=image.shape)\n",
    "    return image + noise\n",
    "\n",
    "# Set paths\n",
    "train_dir = \"/Users/hiteshwaralavala/Desktop/data set in TTV/train\"\n",
    "test_dir = \"/Users/hiteshwaralavala/Desktop/data set in TTV/test\"\n",
    "val_dir = \"/Users/hiteshwaralavala/Desktop/data set in TTV/valid\"\n",
    "image_size = (224, 224)  # DenseNet input size\n",
    "num_classes = 5\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    preprocessing_function=apply_gaussian_noise,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Flow from directory with train_data\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    directory=train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Validation and testing data generators\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory=val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load DenseNet model without top layers\n",
    "base_model = DenseNet121(weights='/Users/hiteshwaralavala/Desktop/data set in TTV/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom top layers with SE block\n",
    "x = base_model.output\n",
    "x = squeeze_excite_block(x)  # Add SE block\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Additional CNN layers\n",
    "cnn_output = Conv2D(64, kernel_size=(3, 3), activation='relu')(base_model.input)\n",
    "cnn_output = MaxPooling2D(pool_size=(2, 2))(cnn_output)\n",
    "cnn_output = Flatten()(cnn_output)  # Add Flatten layer to match shape with GlobalAveragePooling2D output\n",
    "\n",
    "# Concatenate DenseNet and CNN outputs\n",
    "concatenated_output = Concatenate()([x, cnn_output])\n",
    "\n",
    "x = Dense(1024, activation='relu')(concatenated_output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create final model\n",
    "model = Model(inputs=[base_model.input], outputs=[predictions])\n",
    "\n",
    "# Freeze all layers of DenseNet base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "# Train model with early stopping\n",
    "history = model.fit(train_generator, epochs=14, validation_data=val_generator, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate model on test data\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Save model with .h5 extension\n",
    "model.save('/Users/hiteshwaralavala/Desktop/data set in TTV/hybridcnn hybrid_cnn_with_densenet.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
